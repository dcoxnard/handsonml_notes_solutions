{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises on pages 216-217."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which Linear Regression training algorithm can be used on a training set that has millions of features?**\n",
    "\n",
    "An iterative method is needed to train a Linear Regression model on a dataset with millions of features.  This is because the closed-form solution to the OLS problem involves computing the Moooe-Penrose pseudoinverse of the training matrix, $(\\pmb{X}^{\\intercal}\\pmb{X})^{-1}\\pmb{X}^{\\intercal}\\pmb{y}$, which is very computationally inefficient.  Interative methods such a Gradient Descent are more computationally efficient and are guaranteed (under some assumptions) to converge to the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose the features in your traning set have very different scales.  Which algorithms might suffer from this, and how?  What can be done to mitigate this effect?**\n",
    "\n",
    "Any machine learning algorithm that makes comparisons between features on the basis of within-feature Euclidean distance implicitly makes the assumption that the scale on which every feature is measured is the same.  Violating this assumption can lead to undersired results, because those features that are measured on larger scales as affected differently from features measured on smaller scales.  This unequal treatment of the features is almost never desired.\n",
    "\n",
    "For example, when regularizing a Linear Regression model by penalizing some norm of the weights matrix, features measured on a larger scale will be \"penalized\" more than those measured on a smaller scale, leading to the large-scale features to be more drastically reduced towards zero.  This is undesired because typically the goal of using a regularized model is to reduce the weights of features that do not contribute meaninfgully to the predictive power of the model, regardless of the scale on which the features are measured.\n",
    "\n",
    "Algorithms that can suffer from this effect without a data preprocessing step include Ridge Regression, LASSO, and the Elastic Net.  Each one of these models contains a penalty term that penalizes large weights, and thus each model implicitly makes the assumption that all of the features are measured on the same scale.\n",
    "\n",
    "To preprocess the data to fit the regularization assumption, scaling the features is needed. This will guarantee that the features are measured on the same scale, i.e. with a mean of 0 and a standard deviation of 1.  If $\\mu$ is the mean of feature vector $\\pmb{x}$ and $\\sigma$ is its standard deviation, then the preprocessed feature vector $\\tilde{\\pmb{x}}$ is given by: $$ \\tilde{\\pmb{x}} = \\frac{\\pmb{x} - \\mu}{\\sigma} $$.\n",
    "\n",
    "`scikit-learn` implements a `StandardScaler` class that can be used to perform this scaling on all features of an input matrix `X` by calling `X_scaled = StandardScaler().fit_transform(X)`.\n",
    "\n",
    "A nuance of model evaluation that is sometimes forgotten is the need to scale any validation by using the same learned parameters (i.e. $\\mu$ and $\\sigma$ for each sclaed feature), and **not** to learn a new set of parameters to scale the validaiton data. Failure to do so is data leakage: data used to validate the model must be assumed to come from the training data's distribution, and thus must be preprocessed using the parameters learned from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?**\n",
    "\n",
    "Gradient Descent does not run the risk of getting stuck in a local minimum when fitting a Logistic Regression model because the Logistic Regression's loss function is convex.  The convexity of the loss function guarantees that any local minimum is also a global minimum.\n",
    "\n",
    "The loss function used is the negative log likelihood:\n",
    "$$ J(\\pmb{\\theta}) = -\\sum^{m}_{i=1}y_i\\log\\hat{p}_i + (1 - y_i)\\log(1-\\hat{p}_i) $$\n",
    "where\n",
    "$$ \\hat{p_i} = \\mathbb{P}(y_i = 1|\\pmb{\\theta}) = \\frac{1}{1 + e^{-\\pmb{\\theta}^{\\intercal}\\pmb{x}_i}} $$\n",
    "\n",
    "A function $f$ is convex iff: $$ \\forall a, b \\in \\mathbb{R}, 0\\leq \\rho \\leq 1, f((1-\\rho)a + \\rho b) \\geq (1-\\rho)f(a) + \\rho f(b) $$\n",
    "\n",
    "The convexity of the negative log likelihood can be proven by noting that $J\\pmb(\\theta)$ can be rewritten:\n",
    "$$ J(\\pmb{\\theta}) = -\\sum^{m}_{i=1} y_i\\pmb{\\theta}^{\\intercal}\\pmb{x}_i - \\log(1 + e^{\\pmb{\\theta}^{\\intercal}\\pmb{x}_i}) $$\n",
    "\n",
    "and noting that the two terms are both themselves convex functions.  Since the sum of convex functions is itself also convex, the negative log likelihood is convex.\n",
    "\n",
    "Although any local minimum of the loss function is guaranteed to also be a global minimum, it is _not_ guaranteed to be unique.  This situation can arise in fitting a Logistic Regression model when the two classes are linearly separable.  In this situation, a Logistic Regression model may not be stable; this can be addressed by using another model such as Linear Discriminant Analysis.\n",
    "\n",
    "When using Gradient Descent, the learning rate must be set small enough, or must decay to a small value, to ensure that the algorithm does converge to approximately the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do all Gradient Descent algorithms lead to the same model, provided that you run them long enough?**\n",
    "\n",
    "No, models trained with Gradient Descent will not all necessarily converge to the same model, even if the loss function is convex.  The convergence of the Gradent Descent algorithm is affected by the Learning Rate, a parameter that determines the size of the update when using the gradient to update the parameter weights.  If the Learning Rate is too high, the algorithm can repeatedly \"jump across\" the loss function's minimum and fail to converge to a desired model.  Some solutions to this problem include setting the Learning Rate to a low-enough value so that the parameter updates can approach the function's minimum, and decaying the learning rate so that fast progress can be made in the early stages of the algorithm, with finer updates toward the end of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose you use Batch Gradient Desecent and then plot the validation error at every epoch.  If you notice that the validation error consistently increases, what is the likely cause?  How can you fix this?**\n",
    "\n",
    "Increasing validation error is a sign of the model overfitting.  The model's ability to fit the training data gets better at the expense of its ability to generalize well to new data, leading to increasing validation error when testing the model against data that was not used for training.\n",
    "\n",
    "There are many ways to prevent overfitting, many of which depend on the exact model that is being used.  One universal way to reduce the risk of overfitting is to use a larger training set, if possible.  Regularization is a technique that adds a term to  the model's loss function that penalizes more complex models, inducing the model to add complexity only where necessary.  Regularization can be used with linear models or Support Vector machines by adding a penalty term that penalizes large fitted parameter values.  Another way to reduce the risk of overfitting by using a less flexible model is to reduce the number of features used, specifically to discard features that contribute more noise than signal to predictions.  an example of this would be preferring a Linear Regression model over a more flexible Polynomial Regression model.  This reduces the risk of overfitting by trading off a higher model bias for lower model variance.  When working with neural networks, dropping some of the weights during each training epoch prevents the model from becoming too dependent on a small set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is it a good idea to stop Mini-Batch Gradient Descent immediately upon the validation error increasing?**\n",
    "\n",
    "Stopping Mini-Batch gradient Descent immediately when the validation error increases over the previous epoch is too naive of a stopping rule.  The rationale for such an approach is presumably to stop training when the model's generalization ability degrades, and a sustained increase in validation error is a reliable sign that this is happening.  However, an epoch-over-epoch increase in validation error can be due to factors other than overfitting.  For example, the gradient calculated using the mini-batch is noisier than it would be if the gradient were computed against the entire training set.  This noise can cause the Gradient Descent algorithm to update the paramteres weights in such a way that, when trained on a new mini-batch of data, results in slightly higher validation error.  This effect has nothing to do with the generalization ability of the model and everything to do with the noisy parameter update.  Other factors unrelated to generalization ability could cause a temporary increase in validation error, such as a learning rate that is set too high to make effective parameter updates.\n",
    "\n",
    "A better stopping criterion is to wait until the model has seen a sustained degradation of its generalization ability, i.e. a steady rise in the validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which Gradient Descent algorithm (among those discussed in the chapter) will reach the vicinity of the optimal solution the fastest?  Which will actually converge?  How can you make others converge as well?**\n",
    "\n",
    "The chapter presents three variations of the Gradient Descent algorithm.  The first, (regular) Gradient Descent, calculates the gradient of the parameter vector using the entire training data set.  While this yields the most accurate information about the cost function's \"landscape,\" it is costly to compute, espectially if the training set is large.  The second, Stochasti Gradient Descent, calculates the gradient at each epoch with respect to only one randomly chosen training data point.  This is much faster to compute but comes with the drawback that the path taken towards the cost function's minimum is more stochastic, which can mean that the algorithm takes a long time to converge.  The third, Mini-Batch Gradient Descent, represents a compromise between the two extremes: the gradient is computed using a small, randomly-chosen batch of training data points, which yields more accurate gradient information than Stochastic Gradient Descent but is cheaper to compute than using the full dataset.  Mini-Batch Gradient Descent will thus converge faster than Stochastic Gradient Descent.\n",
    "\n",
    "The learning rate hyperparameter plays an important role in a the speed of convergence.  A too-large learning rate will cause the parameter update steps to miss local minima, and may not converge.  A too-small learning rate will more reliably converge, but it can take a long time because more update steps are necessary to reach the minimum.  One way to improve convergence is to decay the learning rate so that, early on in the training, the model is allowed to quickly get closer to the cost function's minimum, while later in the training, the learning rate is small enough to ensure that the model does converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose you are using Polynomial Regression. You plot the learning curves and notice that there is a large gap between the training error and the validation error.  What is happening?  What are three ways to solve this problem?**\n",
    "\n",
    "A large gap between the training and validation error is an indication that the model is not generalizing well, i.e. that it is overfitting.  The issue of overfitting can be dealt with by using a less flexible model.  In the case of Polynomial Regression, this can be accomplished by adding a regularization term to constrain the model, lowering the degree of polynomial features that re used, thereby reducing the number of features, or by performing feature selection to reduce the number of features before constructing higher-order features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose you are using Ridge Regression and you notice that the training and validations errors are both similar and consistently high.  Is the model suffering from bias or variance?  Should you increase the regularization parameter $\\alpha$, or reduce it?**\n",
    "\n",
    "If training and validation loss are similarly high, the model is likely suffering from high bias.  Similar terms are underfitting or a less flexible model than needed.  The assumptions of the model are too restrictive to allow the model to fit the data, so more flexibility is required.  One simple way to increase the flexibility or a regularized model is to decrease the regularization penalty for large weights; this corresponds to using a lower value of $\\alpha$ to better enable the model to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why would one use...**\n",
    "\n",
    "- **...Ridge Regression (i.e. as opposed to non-regularized Linear Regression)?**\n",
    "\n",
    "Ridge Regression is a regularized model, while \"normal\" Linear Regression is unregularized.  This makes Ridge Regression a less flexible model than unregularized Linear Regression.  Thus, Ridge Regression is a better option when it is necessary to prevent the model from overfitting.\n",
    "\n",
    "- **...LASSO instead of Ridge Regression?**\n",
    "\n",
    "LASSS+O imposes a different penatly on the linear model than Ridge Regression; LASSO penalizes the model by the $\\ell_1$ norm while Ridge Regression penalizes it by the $\\ell_2$ norm.  This has the effect of a LASSO model often setting some of the weights to exactly zero where Ridge Regression will typically set weights close to, but not exactly zero.  This can make LASSO useful in a situation that requires feature selection, as this property can be used as one strategy to automatically performing feature selection.\n",
    "\n",
    "- **...Elastic Net instead of LASSO?**\n",
    "\n",
    "Elastic Net represents a continuous tradeoff between LASSO and Ridge Regression.  As such, it is a more flexible model, and thus may be able to fit a certain dataset better.  Elastic Net would likely beat out LASSO if the data includes many highly correlated features: LASSO would reduce almost all of the associated weights to exactly 0, while Elastic Net would keep small nonzero values for the weights, which may be better from a generalization perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose you want to classify pictures as either outdoor or indoor, and either daytime or nighttime.  Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?**\n",
    "\n",
    "This problem must be formulated as two independent binary classifications.  Thus, two Logistic Regressions must be used, not one Softmax Regression.  Logistic Regression is a binary classifier, and using two of these models independently of each other allows for predictions to be made (independently) abount indoor/outdoor and nighttime/daytime.  Softmax Regression by contrast is a tool to be ujsed in multiclass classification, where together the possible prediction classes are mutually exclusive and each training point must be assignedd to exactly one of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Batch Gradient Descent with early stopping for Softmax Regression, without resorting to `sklearn`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iterations = 2000\n",
    "early_stopping = True\n",
    "batch_size = 50\n",
    "\n",
    "n = X.shape[0]\n",
    "p = X.shape[1]\n",
    "K = len(set(y))\n",
    "\n",
    "theta = np.random.randn(p, K)\n",
    "\n",
    "losses = []\n",
    "for i in range(n_iterations):\n",
    "    batch_ix = np.random.choice(n, batch_size, replace=False)\n",
    "    X_batch = X[batch_ix]\n",
    "    y_batch = y[batch_ix]\n",
    "    \n",
    "    # Rows are points\n",
    "    # columns are classes\n",
    "    # Need to check if this implementation of the gradient function is correct.\n",
    "    denom = np.exp(X_batch.dot(theta)).sum(axis=1).reshape(-1, 1)\n",
    "    probabilities = np.exp(X_batch.dot(theta)) / denom\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    # Fancy indexing\n",
    "    # Inspired by: https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array\n",
    "    onehot = np.zeros((y_batch.size, K))\n",
    "    onehot[np.arange(y_batch.size), y_batch] = 1\n",
    "    \n",
    "    loss = probabilities - onehot\n",
    "    losses.append(loss.sum())\n",
    "    gradient = 1/n * loss.T.dot(X_batch).T\n",
    "    update = gradient * learning_rate\n",
    "    theta += update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEDCAYAAAAoWo9tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRc4wDBNwWJEzDMMEHFbkDMMwAef/AdUFpsx6ZXM3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows are points\n",
    "# columns are classes\n",
    "\n",
    "denom = np.exp(X_batch.dot(theta)).sum(axis=1).reshape(-1, 1)\n",
    "probabilities = np.exp(X_batch.dot(theta)) / denom\n",
    "predictions = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.65204312e-01, 1.52565547e-06, 4.34794162e-01],\n",
       "       [6.72893260e-01, 2.89992154e-05, 3.27077741e-01],\n",
       "       [5.69818564e-01, 3.96565029e-02, 3.90524933e-01],\n",
       "       [5.61959583e-01, 3.22805087e-02, 4.05759909e-01]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy indexing\n",
    "# Inspired by: https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array\n",
    "\n",
    "onehot = np.zeros((y_batch.size, y_batch.max()+1))\n",
    "onehot[np.arange(y_batch.size), y_batch] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = onehot - probabilities\n",
    "gradient = loss.T.dot(X_batch).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = gradient * learning_rate\n",
    "theta += update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions on page 952."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl",
   "language": "python",
   "name": "homl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
